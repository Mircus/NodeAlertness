import time
import statistics as stat
import pandas as pd
from pandas_datareader import data as pdr
from pyspark.sql.types import *
from graphframes import *
from graphframes.lib import AggregateMessages as AM
from pyspark.sql.functions import *
import statsmodels.api as sm
import statsmodels.tsa.stattools as ts


def ingest_sp500_data():
    """
    This function retrieves the S&P 500 companies' list from Wikipedia and stores it in a DataFrame
    """
    data = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
    table = data[0]
    tickers = table[0].tolist()[1:]
    tickers.append('^GSPC')  # include SP 500 Index ticker
    return tickers


def download_data(tickers, start, end):
    """
    This function downloads the ticker data
    """
    start_time = time.time()
    data = {}
    for ticker in tickers:
        try:
            data[ticker] = pdr.DataReader(ticker, "yahoo", start=start, end=end)
        except Exception as e:
            print(f"An exception occurred downloading ticker {ticker}. Error: {e}")
    print(f'Downloaded ticker data in: {time.time() - start_time} seconds')
    return data


def cointegration_test(y, X):
    """
    This function performs the cointegration test
    """
    X = sm.add_constant(X)  # adding a constant
    ols_result = sm.OLS(y, X).fit()  # regress one variable on the other 
    beta0 = ols_result.params[0]
    beta1 = ols_result.params[1]
    mean = stat.median(ols_result.resid)
    stdev = stat.stdev(ols_result.resid)
    pvalue = ts.adfuller(ols_result.resid)[1]  # apply Augmented Dickey-Fuller test to check if residual is stationary
    return [beta0, beta1, mean, stdev, pvalue]


def calculate_cointegration(df):
    """
    This function calculates the cointegration for the given dataframe and returns the result
    """
    start_time = time.time()
    result = {}
    for colx in df.columns:
        for coly in df.columns:
            try:
                X = df[colx].values
                Y = df[coly].values
                result[colx, coly] = cointegration_test(Y, X)
            except Exception as e:
                print(f"An exception occurred calculating coint for {colx} {coly}. Error: {e}")
    print(f'Results of cointegration calculated in: {time.time() - start_time} seconds')
    return result


def main():
    # S&P 500 ingestion
    tickers = ingest_sp500_data()


    # Download ticker data
    start_date = '2013-01-01'
    end_date = '2013-12-31'
    ticker_data = download_data(tickers, start_date, end_date)

    # Create dataframe from downloaded data
    pan = pd.Panel(ticker_data)
    df = pan.minor_xs("Close")

    # Create nodes
    df_lastprice = df.tail(1)
    df.drop(df.tail(1).index, inplace=True)
    lst_nodes = [[colx, df_lastprice[colx].values[0]] for colx in df_lastprice.columns]
    df_nodes = pd.DataFrame(lst_nodes, columns=['id', 'lastprice'])

    # Calculate cointegration to build edges
    cointegration_result = calculate_cointegration(df)

    # Prepare cointegration dataframe
    lst_res = [[key[0], key[1]] + res[key] for key in res]
    df_coint = pd.DataFrame(lst_res, columns=['src', 'dst', 'beta0', 'beta1', 'mean', 'stdev', 'pvalue'])

    # Filter out non-significant cointegration results
    df_coint = df_coint[df_coint.pvalue.notnull()]
    df_coint = df_coint[df_coint.pvalue > 0.05]

    # Convert Pandas dataframes to Spark dataframes 
    nodes_schema = StructType([StructField("id", StringType(), True), StructField("lastprice", FloatType(), True)])
    nodes_df = spark.createDataFrame(df_nodes, schema=nodes_schema)

    edges_schema = StructType([StructField("src", StringType(), True), StructField("dst", StringType(), True), 
                                StructField("beta0", FloatType(), True), StructField("beta1", FloatType(), True), 
                                StructField("mean", FloatType(), True), StructField("stdev", FloatType(), True), 
                                StructField("pvalue", FloatType(), True)])
    edges_df = spark.createDataFrame(df_coint, schema=edges_schema)

    # Create graphframe
    cointgraph = GraphFrame(nodes_df, edges_df)

    # Calculate alerts using PREGEL's aggregateMessage() function
    msg_to_dst = ((AM.dst["lastprice"] - (AM.edge["beta1"] * AM.src["lastprice"] + AM.edge["beta0"])) - AM.edge["mean"]) / AM.edge["stdev"]
    agg = cointgraph.aggregateMessages(max(AM.msg).alias("alertStatus"), sendToDst=msg_to_dst)

    agg.show()

if __name__ == "__main__":
    main()

